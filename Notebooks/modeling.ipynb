{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef47fb9-ae7a-48f9-a2af-fed3b1329496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "file1 = r\"data_after_feature_eng_v1.csv\"\n",
    "# Read CSV files into DataFrames\n",
    "\n",
    "# Read CSV files into DataFrames\n",
    "df_full = pd.read_csv(file1)\n",
    "# 3. Tri pour le groupby\n",
    "df_full.sort_values(['variant_id', 'checkout_completed_at'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101c3ca3-20b2-4ae9-b6b1-b0a89f7d6cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Date minimale : 2024-01-01 00:00:00\n",
      "📅 Date maximale : 2025-07-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# 1) Nom de la colonne contenant la date\n",
    "DATE_COL = \"checkout_completed_at\"   # adapte si ta colonne s’appelle autrement, p. ex. \"date\"\n",
    "\n",
    "# 2) Conversion explicite en datetime (sécurise le type)\n",
    "df_full[DATE_COL] = pd.to_datetime(df_full[DATE_COL], errors=\"coerce\")  # errors=\"coerce\" mettra NaT si format invalide\n",
    "\n",
    "# 3) Extraction des bornes\n",
    "date_min = df_full[DATE_COL].min()\n",
    "date_max = df_full[DATE_COL].max()\n",
    "\n",
    "print(\"📅 Date minimale :\", date_min)\n",
    "print(\"📅 Date maximale :\", date_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099b990-f520-4033-8009-feafb18d487a",
   "metadata": {},
   "source": [
    "### Feature Engineering: Creating predictive variables from raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956355dd-85ba-41ef-a261-fde636322f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 total_demand: 104143 outliers détectés\n",
      "🔍 demand_7day_avg: 69469 outliers détectés\n",
      "🔍 lag_1: 73115 outliers détectés\n",
      "🔍 lag_2: 70446 outliers détectés\n",
      "🔍 lag_3: 68124 outliers détectés\n",
      "🔍 lag_7: 60855 outliers détectés\n",
      "🔍 lag_14: 62983 outliers détectés\n",
      "🔍 pct_change_1: 74751 outliers détectés\n",
      "🔍 pct_change_7: 55739 outliers détectés\n",
      "🔍 rolling_mean_7: 26474 outliers détectés\n",
      "🔍 rolling_mean_14: 21408 outliers détectés\n",
      "🔍 rolling_mean_28: 14412 outliers détectés\n",
      "🗑️ Total lignes à supprimer (outliers) : 293431\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "# -----------------------------\n",
    "# 0. Paramètres généraux\n",
    "# -----------------------------\n",
    "START = pd.Timestamp('2024-07-20')\n",
    "END   = pd.Timestamp('2025-07-20')\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Préparer / filtrer df_full\n",
    "# -----------------------------\n",
    "df_full['checkout_completed_at'] = pd.to_datetime(df_full['checkout_completed_at'])\n",
    "\n",
    "df_full = (\n",
    "    df_full\n",
    "      .loc[(df_full['checkout_completed_at'] >= START) &\n",
    "           (df_full['checkout_completed_at'] <= END)]\n",
    "      .copy()\n",
    "      .sort_values(['variant_id', 'checkout_completed_at'])\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Calendrier des événements\n",
    "# -----------------------------\n",
    "period = pd.date_range(START, END, freq='D')          # index journalier\n",
    "\n",
    "event_masks = {\n",
    "    'is_ramadan'       : period.isin(pd.date_range('2025-03-01','2025-03-30')),\n",
    "    'is_aid_el_fitr'   : period == pd.Timestamp('2025-03-30'),\n",
    "    'is_aid_el_adha'   : period == pd.Timestamp('2025-06-06'),\n",
    "    'is_public_holiday': period.isin(pd.to_datetime([\n",
    "        '2024-07-30','2024-08-20','2024-11-18',\n",
    "        '2025-01-01','2025-01-11','2025-05-01'\n",
    "    ])),\n",
    "    'is_back_to_school': period.isin(pd.date_range('2024-08-25','2024-09-15'))\n",
    "}\n",
    "\n",
    "events_df = pd.DataFrame(event_masks, index=period).astype(int)\n",
    "\n",
    "# Merge sur la date\n",
    "df_full = (\n",
    "    df_full\n",
    "      .merge(events_df, how='left',\n",
    "             left_on='checkout_completed_at',\n",
    "             right_index=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "df_full['has_event'] = df_full[list(event_masks)].sum(axis=1).gt(0).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Saisonnalité (jour de l’année)\n",
    "# -----------------------------\n",
    "df_full['day_of_year'] = df_full['checkout_completed_at'].dt.dayofyear\n",
    "df_full['sin_doy'] = np.sin(2*np.pi*df_full['day_of_year']/365)\n",
    "df_full['cos_doy'] = np.cos(2*np.pi*df_full['day_of_year']/365)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Lags, rolling & changements\n",
    "# -----------------------------\n",
    "# Lags classiques\n",
    "for lag in [1, 2, 3, 7, 14]:\n",
    "    df_full[f'lag_{lag}'] = (\n",
    "        df_full.groupby('variant_id')['total_demand']\n",
    "               .shift(lag)\n",
    "    )\n",
    "\n",
    "# Rolling means (après un décalage d’un jour pour éviter la fuite d’info)\n",
    "shifted_td = df_full.groupby('variant_id')['total_demand'].shift(1)\n",
    "\n",
    "for window in [7, 14, 28]:\n",
    "    df_full[f'rolling_mean_{window}'] = (\n",
    "        shifted_td\n",
    "          .rolling(window)\n",
    "          .mean()\n",
    "          .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Écart‑type & médiane sur 7 jours\n",
    "df_full['rolling_std_7'] = (\n",
    "    shifted_td.rolling(7).std().reset_index(level=0, drop=True)\n",
    ")\n",
    "df_full['rolling_median_7'] = (\n",
    "    shifted_td.rolling(7).median().reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Pourcentages de changement\n",
    "df_full['pct_change_1'] = (\n",
    "    df_full.groupby('variant_id')['total_demand']\n",
    "           .pct_change(1, fill_method=None)\n",
    ")\n",
    "df_full['pct_change_7'] = (\n",
    "    df_full.groupby('variant_id')['total_demand']\n",
    "           .pct_change(7, fill_method=None)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Moyennes historiques jour/mois\n",
    "# -----------------------------\n",
    "df_full['day_of_week'] = df_full['checkout_completed_at'].dt.dayofweek\n",
    "df_full['month']       = df_full['checkout_completed_at'].dt.month\n",
    "\n",
    "df_full['dow_demand_avg'] = (\n",
    "    df_full.groupby(['variant_id','day_of_week'])['total_demand']\n",
    "           .transform('mean')\n",
    ")\n",
    "df_full['month_demand_avg'] = (\n",
    "    df_full.groupby(['variant_id','month'])['total_demand']\n",
    "           .transform('mean')\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Ratio de zéros sur 7 jours\n",
    "# -----------------------------\n",
    "df_full['zero_demand_7d'] = (\n",
    "    df_full.groupby('variant_id')['total_demand']\n",
    "           .shift(1).rolling(7)\n",
    "           .apply(lambda x: np.mean(x==0), raw=True)\n",
    "           .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Moyenne glissante 7 jours (min 3 obs.)\n",
    "df_full['demand_7day_avg'] = (\n",
    "    df_full.groupby('variant_id')['total_demand']\n",
    "           .transform(lambda x: x.shift(1).rolling(7, min_periods=3).mean())\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Détection d’outliers (IQR)\n",
    "# -----------------------------\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    outlier_indices = {}\n",
    "    for col in columns:\n",
    "        Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "        outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "        outlier_indices[col] = outliers.index.tolist()\n",
    "        print(f\"🔍 {col}: {len(outliers)} outliers détectés\")\n",
    "    return outlier_indices\n",
    "\n",
    "numeric_cols = [\n",
    "    'total_demand','demand_7day_avg',\n",
    "    'lag_1','lag_2','lag_3','lag_7','lag_14',\n",
    "    'pct_change_1','pct_change_7',\n",
    "    'rolling_mean_7','rolling_mean_14','rolling_mean_28'\n",
    "]\n",
    "\n",
    "outliers_detected = detect_outliers_iqr(df_full, numeric_cols)\n",
    "\n",
    "all_outlier_indices = reduce(\n",
    "    set.union,\n",
    "    (set(v) for v in outliers_detected.values()),\n",
    "    set()\n",
    ")\n",
    "print(f\"🗑️ Total lignes à supprimer (outliers) : {len(all_outlier_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a6797da-530f-4ccf-ba3d-2e336f3da32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. FILTRE magasin n°25\n",
    "df_full = df_full[df_full['store_id'] == 25].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63076d8a-9edf-4fab-8036-50487431921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "855fdf4d-d27b-4418-8a21-d32e412a5916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104,362 séquences créées | % rupture = 46.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.6297 - loss: 0.6500 - val_accuracy: 0.6746 - val_loss: 0.5904\n",
      "Epoch 2/12\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.6486 - loss: 0.6315 - val_accuracy: 0.6746 - val_loss: 0.5901\n",
      "Epoch 3/12\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.6508 - loss: 0.6288 - val_accuracy: 0.6755 - val_loss: 0.5868\n",
      "Epoch 4/12\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.6528 - loss: 0.6277 - val_accuracy: 0.6779 - val_loss: 0.5869\n",
      "Epoch 5/12\n",
      "\u001b[1m653/653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.6514 - loss: 0.6299 - val_accuracy: 0.6768 - val_loss: 0.5892\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# 0) LIBRAIRIES\n",
    "# ---------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "# Vérification essentielle\n",
    "required_cols = [\"variant_id\", \"checkout_completed_at\",\n",
    "                 \"total_demand\", \"stockout_occurred\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes : {missing}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2) AGRÉGATION JOURNALIÈRE + FEATURES\n",
    "# ---------------------------------------------\n",
    "df[\"date\"] = df[\"checkout_completed_at\"].dt.date\n",
    "daily = (df.groupby([\"variant_id\", \"date\"])\n",
    "           .agg(total_demand=(\"total_demand\", \"sum\"),\n",
    "                rupture=(\"stockout_occurred\", \"max\"))\n",
    "           .reset_index()\n",
    "           .sort_values([\"variant_id\", \"date\"]))\n",
    "\n",
    "feat_all = []\n",
    "for vid, g in daily.groupby(\"variant_id\"):\n",
    "    g = g.copy()\n",
    "    g[\"demand\"] = g[\"total_demand\"]\n",
    "    g[\"rupture_past\"] = g[\"rupture\"].shift(1, fill_value=0)\n",
    "    g[\"taux_rupture_7j\"] = g[\"rupture\"].rolling(7, 1).mean()\n",
    "    moy = g[\"demand\"].rolling(7, 1).mean()\n",
    "    g[\"tendance_demand\"] = g[\"demand\"] / (moy + 1)\n",
    "    feat_all.append(g)\n",
    "\n",
    "data = pd.concat(feat_all, ignore_index=True)\n",
    "feature_cols = [\"demand_scaled\", \"rupture_past\",\n",
    "                \"taux_rupture_7j\", \"tendance_demand\"]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3) SÉQUENCES (fenêtre 14 j, horizon 7 j)\n",
    "# ---------------------------------------------\n",
    "seq_len, horizon = 14, 7\n",
    "X, y = [], []\n",
    "scalers = {}\n",
    "\n",
    "for vid, g in data.groupby(\"variant_id\"):\n",
    "    if len(g) < seq_len + horizon:\n",
    "        continue\n",
    "    scaler = MinMaxScaler()\n",
    "    g[\"demand_scaled\"] = scaler.fit_transform(g[[\"demand\"]])\n",
    "    scalers[vid] = scaler\n",
    "\n",
    "    for i in range(len(g) - seq_len - horizon + 1):\n",
    "        X_seq = g.iloc[i:i+seq_len][feature_cols].values\n",
    "        futur = g.iloc[i+seq_len:i+seq_len+horizon][\"rupture\"].values\n",
    "        X.append(X_seq)\n",
    "        y.append(1 if futur.max() else 0)\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "print(f\"{len(X):,} séquences créées | % rupture = {y.mean():.1%}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4) MODÈLE LSTM LÉGER\n",
    "# ---------------------------------------------\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(seq_len, len(feature_cols))),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(1,  activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=Adam(1e-3),\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Gestion du déséquilibre\n",
    "cw = compute_class_weight(\"balanced\", classes=np.unique(y), y=y)\n",
    "class_w = {0: cw[0], 1: cw[1]}\n",
    "\n",
    "early = EarlyStopping(patience=2, monitor=\"val_loss\",\n",
    "                      restore_best_weights=True)\n",
    "\n",
    "model.fit(X, y,\n",
    "          epochs=12,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2,\n",
    "          class_weight=class_w,\n",
    "          shuffle=True,\n",
    "          callbacks=[early],\n",
    "          verbose=1)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5) PRÉDICTION 1-7 juillet 2025\n",
    "# ---------------------------------------------\n",
    "start = pd.Timestamp(\"2025-07-11\").date()\n",
    "preds = []\n",
    "\n",
    "for vid, g in data.groupby(\"variant_id\"):\n",
    "    if vid not in scalers or len(g) < seq_len:\n",
    "        continue\n",
    "    last = g.tail(seq_len).copy()\n",
    "    last[\"demand_scaled\"] = scalers[vid].transform(last[[\"demand\"]])\n",
    "    X_pred = last[feature_cols].values.reshape(1, seq_len, len(feature_cols))\n",
    "    p = float(model.predict(X_pred, verbose=0)[0][0])\n",
    "\n",
    "    if p >= .7:  lvl = \"TRES_HAUT\"\n",
    "    elif p >= .5: lvl = \"HAUT\"\n",
    "    elif p >= .3: lvl = \"MOYEN\"\n",
    "    else:         lvl = \"FAIBLE\"\n",
    "\n",
    "    for d in range(horizon):\n",
    "        preds.append({\"variant_id\": vid,\n",
    "                      \"date\": start + pd.Timedelta(days=d),\n",
    "                      \"rupture_probability\": round(p, 4),\n",
    "                      \"risk_level\": lvl})\n",
    "\n",
    "pred_df = pd.DataFrame(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce5add70-a02c-4ffa-890e-9ffd0579c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 5) PRÉDICTION 1-7 juillet 2025\n",
    "# ---------------------------------------------\n",
    "start = pd.Timestamp(\"2025-07-21\").date()\n",
    "preds = []\n",
    "\n",
    "for vid, g in data.groupby(\"variant_id\"):\n",
    "    if vid not in scalers or len(g) < seq_len:\n",
    "        continue\n",
    "    last = g.tail(seq_len).copy()\n",
    "    last[\"demand_scaled\"] = scalers[vid].transform(last[[\"demand\"]])\n",
    "    X_pred = last[feature_cols].values.reshape(1, seq_len, len(feature_cols))\n",
    "    p = float(model.predict(X_pred, verbose=0)[0][0])\n",
    "\n",
    "    if p >= .7:  lvl = \"TRES_HAUT\"\n",
    "    elif p >= .5: lvl = \"HAUT\"\n",
    "    elif p >= .3: lvl = \"MOYEN\"\n",
    "    else:         lvl = \"FAIBLE\"\n",
    "\n",
    "    for d in range(horizon):\n",
    "        preds.append({\"variant_id\": vid,\n",
    "                      \"date\": start + pd.Timedelta(days=d),\n",
    "                      \"rupture_probability\": round(p, 4),\n",
    "                      \"risk_level\": lvl})\n",
    "\n",
    "pred_df = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2c8c83-ebbc-449d-b339-671ac29028af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier enregistré : predictions_rup_1_7_juillet.csv\n"
     ]
    }
   ],
   "source": [
    "pred_df.to_csv(\"predictions_rup_20__juillet.csv\", index=False)\n",
    "print(\"✅ Fichier enregistré : predictions_rup_1_7_juillet.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
